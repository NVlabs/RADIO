# RADIO Examples

## K-NN Classification

K-NN classification consists of embedding all of the summary vectors produced by the model into a database. We then can classify a new vector given this database by computing a weighted sum of the $k$ nearest vectors in the database.

We follow the algorithm in [Unsupervised Feature Learning via Non-Parametric Instance Discrimination](https://arxiv.org/pdf/1805.01978.pdf) (Section 3.4) for the weighting. Let $\mathbf{q} \in \mathbb{R}^C$ be the query vector, and $\mathbf{D} \in \mathbb{R}^{N \times C}$ with $N$ being the size of the training set, and $C$ the dimension of the summary vectors. Let $\mathbf{d}_i$ be the $i$-th row of $\mathbf{D}$, let $s_i = \cos(\mathbf{d}_i, \mathbf{q})$ be the cosine similarity between the database vector and the query vector, and let $c_i$ be the class corresponding to vector $\mathbf{d}_i$.

The top $k$ nearest neighbors, denoted by $\mathcal{N}_k$, is the set of vectors of size $k$ with highest cosine similarity over $\mathbf{D}$. Then, class $c$ gets a total weight
$$
\begin{aligned}
    \alpha_i &= e^{s_i / \tau} \\
    w_c &= \sum_{i \in \mathcal{N}_k} \mathbb{1}\left(c_i = c\right) \cdot \alpha_i \\
    c_{\mathbf{q}} &= \argmax_c w_c
\end{aligned}
$$

Following Wu et.al. we set $\tau = 0.07$, thus $c_{\mathbf{q}}$ is the predicted class. Note that this is the same formulation that [DINO](http://arxiv.org/abs/2104.14294) and [DINOv2](http://arxiv.org/abs/2304.07193) use.

We argue that K-NN classification is a great evaluation protocol for holistic image tasks because it works for any vision model that can produce a summary vector for an image, and thus it easily allows us to compare, for example, CLIP vs DINOv2 vs SAM vs etc. The technique is not zero-shot like CLIP in the sense that it still requires a scan through the training set, however, it's also a training-free technique wrt the model.

### Evaluation Script

We provide an implementation of the K-NN algorithm with with the `knn_classification.py` script, which uses the HuggingFace Datasets library to make evaluation simple. This script works in both single-GPU and multi-GPU setups. For multi-GPU, replace `python` with `torchrun`.

```Bash
# Food101
python knn_classification.py --dataset food101

# Oxford Pets
python knn_classification.py --dataset "jonathancui/oxford-pets" --eval-split test

# ImageNet-1K
torchrun --nproc-per-node 8 knn_classification.py --dataset imagenet-1k

# ImageNet Sketch
# This usage is a bit peculiar. ImageNet-Sketch is a different evaluation set for ImageNet-1K, so we
#   use the same training database as ImageNet-1K.
torchrun --nproc-per-node 8 knn_classification.py --dataset imagenet-1k --eval-dataset imagenet_sketch --eval-split train
```

Owing to the flexibility of RADIO, we also provide the ability to flexibly define the input resolution to the model. The default is that the image is resized to 378px on the smaller dimension, and then we center-crop to 378px on the larger dimension, resulting in a (378, 378) image being fed to the model.

Alternatively, you can use a different center cropped resolution by specifying `--resolution 224 224` (or any other value divisible `--resize-multiple`, which should be the patch size of the RADIO model).

RADIO also supports non-square inputs, which you can configure using only a single resolution value, e.g. `--resolution 378`, which means that the smaller image dimension will be resized to 378px, and the larger dimension resized aspect preserving.

### Patch Position Predictor

This example showcases a property of RADIO features, i.e. that patch tokens at the output of the model retain information about their position within the image.
This is useful to know, since some downstream applications (e.g. Visual Question Answering) can benefit from spatial awareness, for example to answer questions
such as "Q: What is on the left of the table? A: A person."

The position predictor consumes frozen RADIO features and applies a linear transformation of individual patch tokens (this is implemented using a 2-channel 1x1
conv2d with stride 1) in order to predict the coordinates of each token.

Below is a sample invocation of the test script (use `--help` for command-line options):

```
torchrun --nproc-per-node 8 position_predictor.py
```

At the end of training, the average L1 distance between predictions and x/y coordinates (in the -1/+1 range) is expected to be in the vicinity of 0.02,
indicating good spatial awareness.
